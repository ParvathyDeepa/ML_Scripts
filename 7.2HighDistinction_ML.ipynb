{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2HD Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DateTime  Temperature  Humidity  Wind Speed  general diffuse flows  \\\n",
      "0  1/1/2017 0:00        6.559      73.8       0.083                  0.051   \n",
      "1  1/1/2017 0:10        6.414      74.5       0.083                  0.070   \n",
      "2  1/1/2017 0:20        6.313      74.5       0.080                  0.062   \n",
      "3  1/1/2017 0:30        6.121      75.0       0.083                  0.091   \n",
      "4  1/1/2017 0:40        5.921      75.7       0.081                  0.048   \n",
      "\n",
      "   diffuse flows  Zone 1 Power Consumption  Zone 2  Power Consumption  \\\n",
      "0          0.119               34055.69620                16128.87538   \n",
      "1          0.085               29814.68354                19375.07599   \n",
      "2          0.100               29128.10127                19006.68693   \n",
      "3          0.096               28228.86076                18361.09422   \n",
      "4          0.085               27335.69620                17872.34043   \n",
      "\n",
      "   Zone 3  Power Consumption  \n",
      "0                20240.96386  \n",
      "1                20131.08434  \n",
      "2                19668.43373  \n",
      "3                18899.27711  \n",
      "4                18442.40964  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#extracting data\n",
    "main_data = pd.read_csv('dataset_city.csv')\n",
    "\n",
    "print(main_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52416 entries, 0 to 52415\n",
      "Data columns (total 11 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   DateTime                   52416 non-null  datetime64[ns]\n",
      " 1   Temperature                52416 non-null  float64       \n",
      " 2   Humidity                   52416 non-null  float64       \n",
      " 3   Wind Speed                 52416 non-null  float64       \n",
      " 4   general diffuse flows      52416 non-null  float64       \n",
      " 5   diffuse flows              52416 non-null  float64       \n",
      " 6   Zone 1 Power Consumption   52416 non-null  float64       \n",
      " 7   Zone 2  Power Consumption  52416 non-null  float64       \n",
      " 8   Zone 3  Power Consumption  52416 non-null  float64       \n",
      " 9   UnixTimestamp              52416 non-null  float64       \n",
      " 10  aggregated_consumption     52416 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10)\n",
      "memory usage: 4.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Analysing the dataframe\n",
    "print(main_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#creating a unix timestamp value for feeding into models.\n",
    "main_data['DateTime'] = pd.to_datetime(main_data['DateTime'], format='%m/%d/%Y %H:%M')\n",
    "main_data['UnixTimestamp'] = main_data['DateTime'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: (52416, 6)\n",
      "Quads target shape: (52416,)\n",
      "Smir target shape: (52416,)\n",
      "Boussafou target shape: (52416,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# Extract features and targets\n",
    "features = ['UnixTimestamp', 'Temperature', 'Humidity', 'Wind Speed', 'diffuse flows', 'general diffuse flows']\n",
    "targets = ['Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']\n",
    "main_data['aggregated_consumption'] = (main_data['Zone 1 Power Consumption'] + main_data['Zone 2  Power Consumption'] + main_data['Zone 3  Power Consumption'])\n",
    "\n",
    "X = main_data[features]\n",
    "y_quads = main_data[targets[0]]\n",
    "y_smir = main_data[targets[1]]\n",
    "y_boussafou = main_data[targets[2]]\n",
    "y_aggrtd = main_data['aggregated_consumption']\n",
    "\n",
    "# Checking the dimensions of the features and targets\n",
    "print(\"Input features shape:\", X.shape)\n",
    "print(\"Quads target shape:\", y_quads.shape)\n",
    "print(\"Smir target shape:\", y_smir.shape)\n",
    "print(\"Boussafou target shape:\", y_boussafou.shape)\n",
    "print(\"Aggregate target shape:\", y_aggrtd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in input features: 0\n",
      "Missing values in Quads target: 0\n",
      "Missing values in Smir target: 0\n",
      "Missing values in Boussafou target: 0\n",
      "Missing values in Aggregated target: 0\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values in features and targets\n",
    "print(\"Missing values in input features:\", X.isnull().sum().sum())\n",
    "print(\"Missing values in Quads target:\", y_quads.isnull().sum())\n",
    "print(\"Missing values in Smir target:\", y_smir.isnull().sum())\n",
    "print(\"Missing values in Boussafou target:\", y_boussafou.isnull().sum())\n",
    "print(\"Missing values in Aggregated target:\", y_aggrtd.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table II implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229/1229 [==============================] - 2s 1ms/step - loss: 1093355136.0000 - mean_absolute_error: 32288.8535\n",
      "1229/1229 [==============================] - 1s 896us/step\n",
      "410/410 [==============================] - 0s 1ms/step\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 463305152.0000 - mean_absolute_error: 20888.7461\n",
      "1229/1229 [==============================] - 1s 881us/step\n",
      "410/410 [==============================] - 0s 1ms/step\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 349979456.0000 - mean_absolute_error: 17501.3633\n",
      "1229/1229 [==============================] - 1s 892us/step\n",
      "410/410 [==============================] - 0s 888us/step\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 5261942784.0000 - mean_absolute_error: 70493.8984\n",
      "1229/1229 [==============================] - 1s 981us/step\n",
      "410/410 [==============================] - 0s 998us/step\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_quads_train, y_quads_test, y_smir_train, y_smir_test, y_boussafou_train, y_boussafou_test, y_aggrtd_train, y_aggrtd_test = train_test_split(X, y_quads, y_smir, y_boussafou,y_aggrtd, test_size=0.25, random_state=42)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def create_ffnn(neurons=10, activation='selu'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(neurons, activation=activation, input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=None, min_samples_split=10, min_samples_leaf=10, max_features=9),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=30, max_features=7, min_samples_split=2, min_samples_leaf=1),\n",
    "    'SVR': SVR(kernel='rbf', C=10, gamma=0.01),\n",
    "    'FFNN': create_ffnn(neurons=10, activation='selu')\n",
    "}\n",
    "\n",
    "# Train and evaluate models for each zone\n",
    "zones = ['Quads', 'Smir', 'Boussafou', 'Aggregated']\n",
    "results = []\n",
    "\n",
    "for zone in zones:\n",
    "    if zone == 'Quads':\n",
    "        y_train = y_quads_train\n",
    "        y_test = y_quads_test\n",
    "    elif zone == 'Smir':\n",
    "        y_train = y_smir_train\n",
    "        y_test = y_smir_test\n",
    "    elif zone == 'Boussafou':\n",
    "        y_train = y_boussafou_train\n",
    "        y_test = y_boussafou_test\n",
    "    else:  # Aggregated\n",
    "        y_train = y_aggrtd_train\n",
    "        y_test = y_aggrtd_test\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        results.append([zone, name, train_rmse, test_rmse, train_mae, test_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Zone       | Model             |   RMSE (Train) |   RMSE (Test) |   MAE (Train) |   MAE (Test) |\n",
      "+============+===================+================+===============+===============+==============+\n",
      "| Quads      | Linear Regression |       6281.93  |       6281.03 |      5154.09  |     5165.6   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Quads      | Decision Tree     |       2689.88  |       3694.92 |      1638.07  |     2233.77  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Quads      | Random Forest     |        952.923 |       2397.09 |       569.335 |     1481.15  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Quads      | SVR               |       7041.52  |       7058.75 |      5829.59  |     5854.35  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Quads      | FFNN              |      33019     |      33130.2  |     32241.2   |    32351.9   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Smir       | Linear Regression |       4574.6   |       4568.1  |      3740.71  |     3742.77  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Smir       | Decision Tree     |       1861.43  |       2560.82 |      1137.29  |     1556.69  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Smir       | Random Forest     |        622.077 |       1573.8  |       371.782 |      980.178 |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Smir       | SVR               |       5098.94  |       5109.7  |      4156.03  |     4152.46  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Smir       | FFNN              |      21449     |      21518.7  |     20812     |    20881.9   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Boussafou  | Linear Regression |       4836.03  |       4800.73 |      3859.75  |     3835.84  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Boussafou  | Decision Tree     |       1818.63  |       2486.56 |      1087.33  |     1492.68  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Boussafou  | Random Forest     |        635.343 |       1588.09 |       366.39  |      968.582 |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Boussafou  | SVR               |       6651.1   |       6700.04 |      4893.71  |     4956.6   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Boussafou  | FFNN              |      18605.7   |      18718.6  |     17393.4   |    17503.4   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated | Linear Regression |      14537.5   |      14486.1  |     11852.5   |    11851.6   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated | Decision Tree     |       5945.81  |       8056.12 |      3557.59  |     4807.63  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated | Random Forest     |       2100.27  |       5227.36 |      1237.58  |     3244.28  |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated | SVR               |      17095.7   |      17172    |     13709.8   |    13797.9   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated | FFNN              |      72341.7   |      72636.3  |     70292.7   |    70583.5   |\n",
      "+------------+-------------------+----------------+---------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "headers = [\"Zone\", \"Model\", \"RMSE (Train)\", \"RMSE (Test)\", \"MAE (Train)\", \"MAE (Test)\"]\n",
    "print(tabulate(results, headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table IV implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 878us/step\n",
      "69/69 [==============================] - 0s 1ms/step\n",
      "205/205 [==============================] - 0s 864us/step\n",
      "69/69 [==============================] - 0s 911us/step\n",
      "205/205 [==============================] - 0s 898us/step\n",
      "69/69 [==============================] - 0s 904us/step\n",
      "205/205 [==============================] - 0s 839us/step\n",
      "69/69 [==============================] - 0s 973us/step\n",
      "1-Hour Prediction Results:\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Model             | Zone       |   RMSE (Train) |   RMSE (Test) |   MAE (Train) |   MAE (Test) |\n",
      "+===================+============+================+===============+===============+==============+\n",
      "| Linear Regression | Zone 1     |        6272.76 |       6298.91 |       5165.35 |      5180.86 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Linear Regression | Zone 2     |        4698.17 |       4691.52 |       3796.74 |      3792.48 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Linear Regression | Zone 3     |        5553.23 |       5629.11 |       4425.35 |      4511.24 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Linear Regression | Aggregated |       14550.9  |      14736.5  |      11900.1  |     12055.7  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Decision Tree     | Zone 1     |        4371.2  |       5978.32 |       3093.89 |      4287.04 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Decision Tree     | Zone 2     |        3231.14 |       4429.15 |       2389.61 |      3300.74 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Decision Tree     | Zone 3     |        3448.22 |       4888.3  |       2502.34 |      3625.53 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Decision Tree     | Aggregated |        6276.43 |      15711.6  |       3919.5  |     10786.7  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Random Forest     | Zone 1     |        1985.05 |       5203.2  |       1439.99 |      3852.72 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Random Forest     | Zone 2     |        1722.13 |       3888.54 |       1221.68 |      2898.81 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Random Forest     | Zone 3     |        3478.75 |       4341.22 |       2598.64 |      3284.78 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| Random Forest     | Aggregated |        4374.57 |      11800.1  |       3161.63 |      8688.47 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| SVR               | Zone 1     |        7080.59 |       6959.05 |       5901.53 |      5750.31 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| SVR               | Zone 2     |        5192.57 |       5103.32 |       4242    |      4147.86 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| SVR               | Zone 3     |        6146.32 |       6167.97 |       4596.94 |      4646.22 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| SVR               | Aggregated |       17129.8  |      16922.3  |      13822.3  |     13501.7  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| FFNN              | Zone 1     |        7099.02 |       6976.39 |       5919.43 |      5767.04 |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| FFNN              | Zone 2     |       21614.1  |      21656.8  |      20982.6  |     21049.3  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| FFNN              | Zone 3     |       18820.8  |      18960.8  |      17628.6  |     17793.4  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n",
      "| FFNN              | Aggregated |       73058.8  |      73334    |      71036.8  |     71376.5  |\n",
      "+-------------------+------------+----------------+---------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset_city.csv')\n",
    "\n",
    "# Convert 'DateTime' column to datetime format and set it as the index\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data = data.set_index('DateTime')\n",
    "\n",
    "# Resample the data to 1-hour intervals\n",
    "data = data.resample('H').mean()\n",
    "\n",
    "# Split the dataset into features and target variables for each zone and aggregated distribution\n",
    "X = data[['Temperature', 'Humidity', 'Wind Speed', 'general diffuse flows', 'diffuse flows']]\n",
    "# 'Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption'\n",
    "y_zones = [data['Zone 1 Power Consumption'] ,data['Zone 2  Power Consumption'], data['Zone 3  Power Consumption']]\n",
    "y_aggregated = data[['Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']].sum(axis=1)\n",
    "\n",
    "# Apply Min-Max scaling to the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training (75%) and testing (25%) sets for each zone and aggregated distribution\n",
    "train_test_data = [train_test_split(X_scaled, y, test_size=0.25, random_state=42) for y in y_zones + [y_aggregated]]\n",
    "\n",
    "# Define the hyperparameter values for hourly predictions\n",
    "param_values_hourly = {\n",
    "    'Decision Tree': [\n",
    "        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 5},\n",
    "        {'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 7},\n",
    "        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 9},\n",
    "        {'max_depth': None, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 9}\n",
    "    ],\n",
    "    'Random Forest': [\n",
    "        {'n_estimators': 50, 'max_features': 3, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
    "        {'n_estimators': 10, 'max_features': 7, 'min_samples_split': 3, 'min_samples_leaf': 1},\n",
    "        {'n_estimators': 10, 'max_features': 7, 'min_samples_split': 3, 'min_samples_leaf': 10},\n",
    "        {'n_estimators': 100, 'max_features': 5, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
    "    ],\n",
    "    'SVR': [\n",
    "        {'kernel': 'rbf', 'C': 10, 'gamma': 0.01},\n",
    "        {'kernel': 'rbf', 'C': 1, 'gamma': 0.01},\n",
    "        {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01},\n",
    "        {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}\n",
    "    ],\n",
    "    'FFNN': [\n",
    "        {'batch_size': 100, 'epochs': 100, 'optimizer': SGD(learning_rate=0.001), 'neurons': 25, 'activation': 'relu'},\n",
    "        {'batch_size': 350, 'epochs': 100, 'optimizer': Adam(learning_rate=0.001), 'neurons': 4, 'activation': 'selu'},\n",
    "        {'batch_size': 250, 'epochs': 100, 'optimizer': Adam(learning_rate=0.001), 'neurons': 8, 'activation': 'selu'},\n",
    "        {'batch_size': 250, 'epochs': 100, 'optimizer': Adam(learning_rate=0.001), 'neurons': 4, 'activation': 'selu'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the machine learning models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models for each zone and aggregated distribution\n",
    "table_data = []\n",
    "headers = [\"Model\", \"Zone\", \"RMSE (Train)\", \"RMSE (Test)\", \"MAE (Train)\", \"MAE (Test)\"]\n",
    "zones = ['Zone 1', 'Zone 2', 'Zone 3', 'Aggregated']\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(train_test_data):\n",
    "        if model_name in param_values_hourly:\n",
    "            model.set_params(**param_values_hourly[model_name][i])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "        table_data.append([model_name, zones[i], train_rmse, test_rmse, train_mae, test_mae])\n",
    "\n",
    "# Train and evaluate the FFNN model for each zone and aggregated distribution\n",
    "for i, (X_train, X_test, y_train, y_test) in enumerate(train_test_data):\n",
    "    params = param_values_hourly['FFNN'][i]\n",
    "    ffnn_model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(params['neurons'], activation=params['activation']),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    ffnn_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    ffnn_model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "\n",
    "    y_pred_train = ffnn_model.predict(X_train)\n",
    "    y_pred_test = ffnn_model.predict(X_test)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "    table_data.append(['FFNN', zones[i], train_rmse, test_rmse, train_mae, test_mae])\n",
    "\n",
    "print(\"1-Hour Prediction Results:\")\n",
    "print(tabulate(table_data, headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Results:\n",
      "+------------+----------------+---------------+---------------+--------------+\n",
      "| Zone       |   RMSE (Train) |   RMSE (Test) |   MAE (Train) |   MAE (Test) |\n",
      "+============+================+===============+===============+==============+\n",
      "| Zone 1     |       1022.21  |       1484.98 |       725.768 |     1054.87  |\n",
      "+------------+----------------+---------------+---------------+--------------+\n",
      "| Zone 2     |        703.786 |       1105.98 |       499.876 |      801.215 |\n",
      "+------------+----------------+---------------+---------------+--------------+\n",
      "| Zone 3     |        746.257 |       1206.62 |       511.247 |      785.297 |\n",
      "+------------+----------------+---------------+---------------+--------------+\n",
      "| Aggregated |       2098.84  |       3191.03 |      1475.93  |     2193.77  |\n",
      "+------------+----------------+---------------+---------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset\n",
    "new_data = pd.read_csv('dataset_city.csv')\n",
    "\n",
    "# Convert 'DateTime' column to datetime format and set it as the index\n",
    "new_data['DateTime'] = pd.to_datetime(new_data['DateTime'])\n",
    "new_data = new_data.set_index('DateTime')\n",
    "\n",
    "# Resample the new_data to 1-hour intervals\n",
    "new_data = new_data.resample('H').mean()\n",
    "\n",
    "# Feature engineering\n",
    "new_data['hour'] = new_data.index.hour\n",
    "new_data['day_of_week'] = new_data.index.dayofweek\n",
    "new_data['month'] = new_data.index.month\n",
    "\n",
    "#created new compuned data to form new features\n",
    "new_data['Temperature_Humidity'] = new_data['Temperature'] * new_data['Humidity']\n",
    "new_data['WindSpeed_GeneralDiffuseFlows'] = new_data['Wind Speed'] * new_data['general diffuse flows']\n",
    "\n",
    "# Split the dataset into features and target variables for each zone and aggregated distribution\n",
    "X = new_data[['Temperature', 'Humidity', 'Wind Speed', 'general diffuse flows', 'diffuse flows',\n",
    "          'hour', 'day_of_week', 'month', 'Temperature_Humidity', 'WindSpeed_GeneralDiffuseFlows']]\n",
    "y_zones = [new_data['Zone 1 Power Consumption'] ,new_data['Zone 2  Power Consumption'], new_data['Zone 3  Power Consumption']]\n",
    "y_aggregated = new_data[['Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']].sum(axis=1)\n",
    "\n",
    "# Apply Min-Max scaling to the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training (75%) and testing (25%) sets for each zone and aggregated distribution\n",
    "train_test_data = [train_test_split(X_scaled, y, test_size=0.25, random_state=42) for y in y_zones + [y_aggregated]]\n",
    "\n",
    "# Define the base models and their hyperparameters\n",
    "base_models = [\n",
    "    ('RF', RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=2)),\n",
    "    ('XGB', XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, subsample=0.7)),\n",
    "    ('SVR', SVR(C=1, gamma=0.1))\n",
    "]\n",
    "\n",
    "# Train and evaluate the ensemble model for each zone and aggregated distribution\n",
    "table_data = []\n",
    "headers = [\"Zone\", \"RMSE (Train)\", \"RMSE (Test)\", \"MAE (Train)\", \"MAE (Test)\"]\n",
    "zones = ['Zone 1', 'Zone 2', 'Zone 3', 'Aggregated']\n",
    "\n",
    "for i, (X_train, X_test, y_train, y_test) in enumerate(train_test_data):\n",
    "    # Create the stacking ensemble\n",
    "    ensemble = []\n",
    "\n",
    "    # Train the base models\n",
    "    for name, model in base_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        ensemble.append((name, model))\n",
    "\n",
    "    # Make predictions using the base models\n",
    "    base_predictions = []\n",
    "    for name, model in ensemble:\n",
    "        base_predictions.append(model.predict(X_test))\n",
    "\n",
    "    # Stack the base model predictions and train the meta-model\n",
    "    stacked_predictions = np.column_stack(base_predictions)\n",
    "    meta_model = LinearRegression()\n",
    "    meta_model.fit(stacked_predictions, y_test)\n",
    "\n",
    "    # Make predictions using the ensemble model\n",
    "    ensemble_predictions_train = []\n",
    "    ensemble_predictions_test = []\n",
    "    for name, model in ensemble:\n",
    "        ensemble_predictions_train.append(model.predict(X_train))\n",
    "        ensemble_predictions_test.append(model.predict(X_test))\n",
    "\n",
    "    ensemble_predictions_train = np.column_stack(ensemble_predictions_train)\n",
    "    ensemble_predictions_test = np.column_stack(ensemble_predictions_test)\n",
    "\n",
    "    ensemble_predictions_train = meta_model.predict(ensemble_predictions_train)\n",
    "    ensemble_predictions_test = meta_model.predict(ensemble_predictions_test)\n",
    "\n",
    "    # Evaluate the ensemble model\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, ensemble_predictions_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, ensemble_predictions_test))\n",
    "    train_mae = mean_absolute_error(y_train, ensemble_predictions_train)\n",
    "    test_mae = mean_absolute_error(y_test, ensemble_predictions_test)\n",
    "\n",
    "    table_data.append([zones[i], train_rmse, test_rmse, train_mae, test_mae])\n",
    "\n",
    "print(\"Ensemble Model Results:\")\n",
    "print(tabulate(table_data, headers, tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deakin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
